{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Change tensorflow version from 2.6.4 to 2.9.1\n\nI noticed that the default version of tensorflow (2.6.4) has a problem of printing unecessary clean up messages. for this reason I upgraded to tensorflow 2.9.1","metadata":{}},{"cell_type":"code","source":"!pip uninstall tensorflow --yes\n!pip install tensorflow_decision_forests\n!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 --yes\n!pip install kerassurgeon ","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:56:11.767715Z","iopub.execute_input":"2022-11-07T06:56:11.768203Z","iopub.status.idle":"2022-11-07T06:57:45.513752Z","shell.execute_reply.started":"2022-11-07T06:56:11.768110Z","shell.execute_reply":"2022-11-07T06:57:45.512236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Library\nhere are the required packages/modules we will use to perform the tasks","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport gc\nimport numpy as np\nimport pandas as pd\nimport glob\nimport pathlib\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\n#from keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow.keras import datasets, layers, models,Input,Model \nimport tensorflow_datasets as tfds\nfrom keras.models import Sequential\nfrom keras.layers import  Bidirectional, Conv2D, BatchNormalization, MaxPooling2D, Flatten, LSTM, Dense, Lambda, Dropout,Reshape\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom tensorflow.keras.metrics import Accuracy, Recall,Precision\nfrom sklearn.tree import DecisionTreeClassifier as Decisiontree\nfrom sklearn.svm import SVC as Supportvectorclassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport time\nfrom functools import reduce\nfrom kerassurgeon.operations import delete_layer, insert_layer\nfrom keras.utils import to_categorical\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:57:45.516825Z","iopub.execute_input":"2022-11-07T06:57:45.517200Z","iopub.status.idle":"2022-11-07T06:57:52.763956Z","shell.execute_reply.started":"2022-11-07T06:57:45.517155Z","shell.execute_reply":"2022-11-07T06:57:52.762869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just checking if tensorflow was sucessfully upgraded.","metadata":{}},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:57:52.765480Z","iopub.execute_input":"2022-11-07T06:57:52.766153Z","iopub.status.idle":"2022-11-07T06:57:52.771972Z","shell.execute_reply.started":"2022-11-07T06:57:52.766118Z","shell.execute_reply":"2022-11-07T06:57:52.771054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some parameter settings\n\nbatch_size=32\nimage_height=299\nimage_width=299\nn=1000\nepochs=100","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:57:52.774990Z","iopub.execute_input":"2022-11-07T06:57:52.775350Z","iopub.status.idle":"2022-11-07T06:57:53.854422Z","shell.execute_reply.started":"2022-11-07T06:57:52.775318Z","shell.execute_reply":"2022-11-07T06:57:53.853083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to get labs \ndef fetch_labels(filepath):\n    return str(filepath).split('/')[-2]\n","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:57:53.856505Z","iopub.execute_input":"2022-11-07T06:57:53.857150Z","iopub.status.idle":"2022-11-07T06:57:53.869427Z","shell.execute_reply.started":"2022-11-07T06:57:53.857103Z","shell.execute_reply":"2022-11-07T06:57:53.868501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is our model, This section is the feature extraction part, each block  contains convolution layers normalization layers and maximum pooling layers.","metadata":{}},{"cell_type":"code","source":"model=Sequential(name=\"Full_Model\")\n# Block 1\nmodel.add(Input(shape=(image_height,image_width,1),name=\"input\"))\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu',name=\"block1_conv_1\"))\nmodel.add(BatchNormalization(name=\"block1_batch_normalization1\"))\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu',name=\"block1_conv_2\"))\nmodel.add(BatchNormalization(name=\"block1_batch_normalization2\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block1_maxpool\"))\n\n# Block 2\n\nmodel.add(Conv2D(128, (3, 3), padding='same', activation='relu',name=\"block2_conv_1\"))\nmodel.add(BatchNormalization(name=\"block2_batch_normalization1\"))\nmodel.add(Conv2D(128, (3, 3), padding='same', activation='relu',name=\"block2_conv_2\"))\nmodel.add(BatchNormalization(name=\"block2_batch_normalization2\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block2_maxpool\"))\n          \n# Block 3\nmodel.add(Conv2D(256, (3, 3), padding='same', activation='relu',name=\"block3_conv_1\"))\nmodel.add(BatchNormalization(name=\"block3_batch_normalization1\"))\nmodel.add(Conv2D(256, (3, 3), padding='same', activation='relu',name=\"block3_conv_2\"))\nmodel.add(BatchNormalization(name=\"block3_batch_normalization2\"))\nmodel.add(Conv2D(256, (3, 3), padding='same', activation='relu',name=\"block3_conv_3\"))\nmodel.add(BatchNormalization(name=\"block3_batch_normalization3\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2),name=\"block3_maxpool\"))\n\n# Block 4\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block4_conv_1\"))\nmodel.add(BatchNormalization(name=\"block4_batch_normalization1\"))\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block4_conv_2\"))\nmodel.add(BatchNormalization(name=\"block4_batch_normalization2\"))\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block4_conv_3\"))\nmodel.add(BatchNormalization(name=\"block4_batch_normalization3\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block4_maxpool\"))\n\n# fifth convolution layer\n\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block5_conv_1\"))\nmodel.add(BatchNormalization(name=\"block5_batch_normalization1\"))\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block5_conv_2\"))\nmodel.add(BatchNormalization(name=\"block5_batch_normalization2\"))\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block5_conv_3\"))\nmodel.add(BatchNormalization(name=\"block5_batch_normalization3\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block5_maxpool\"))\n#  flatten\nmodel.add(Flatten(name=\"flatten_layer\"))","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:57:53.870662Z","iopub.execute_input":"2022-11-07T06:57:53.871022Z","iopub.status.idle":"2022-11-07T06:57:54.534518Z","shell.execute_reply.started":"2022-11-07T06:57:53.870990Z","shell.execute_reply":"2022-11-07T06:57:54.533370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding dense layers\n\nHere we will connect the feature extraction part with the ANN classifier to produce a single model like shown on the figure above.","metadata":{}},{"cell_type":"code","source":"# Dense connected layers\nmodel.add(Dense(units=64,activation=\"relu\"))\nmodel.add(Dense(units=64,activation=\"relu\"))\nmodel.add(Dense(units=2, activation=\"softmax\"))","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:57:54.536007Z","iopub.execute_input":"2022-11-07T06:57:54.536745Z","iopub.status.idle":"2022-11-07T06:57:54.614765Z","shell.execute_reply.started":"2022-11-07T06:57:54.536700Z","shell.execute_reply":"2022-11-07T06:57:54.613805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:57:54.616170Z","iopub.execute_input":"2022-11-07T06:57:54.617209Z","iopub.status.idle":"2022-11-07T06:57:54.622987Z","shell.execute_reply.started":"2022-11-07T06:57:54.617168Z","shell.execute_reply":"2022-11-07T06:57:54.621545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compile and fit the ANN model. \n\nA sparse categoricalCrossentropy will be used.","metadata":{}},{"cell_type":"code","source":"model.compile(\n  optimizer='adam',\n  loss=\"categorical_crossentropy\",\n  metrics=[\"accuracy\",\n           tf.keras.metrics.Recall(name=\"Sensitivity\",class_id=0),\n           tf.keras.metrics.Recall(name=\"Specificity\",class_id=1),\n           tf.keras.metrics.Precision(name=\"Precision\",class_id=0),\n          tfa.metrics.F1Score(num_classes=2, average=\"micro\")])","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:57:54.624487Z","iopub.execute_input":"2022-11-07T06:57:54.624973Z","iopub.status.idle":"2022-11-07T06:57:54.666260Z","shell.execute_reply.started":"2022-11-07T06:57:54.624938Z","shell.execute_reply":"2022-11-07T06:57:54.665093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make the code we will use the first 20 Batches. But later we will run the code with the full data","metadata":{}},{"cell_type":"markdown","source":"## Dataset 1: Corona hack\n\nJust like the study dataset this benchmark data consists of x-ray images from several study groups. The control group includes x-ray scans from **normal persons (1576)**, and the experimental group has **58 x-ray scans from COVID -19 infected persons**. The other group contains **ARDS, SARS, and streptococcus bacteria**. The choice of the data was motivated by the presence of competing outcomes (it consists of x-ray images for other respiratory infections, which are more like COVID-19 and pneumonia infections). With this type of data, the ability of the algorithms to detect and identify COVID-19 in presence of other chest infections can be investigated. \n\nDatalink: https://www.kaggle.com/datasets/praveengovi/coronahack-chest-xraydataset","metadata":{}},{"cell_type":"markdown","source":"In total the data consists of **5910 images**, and which are split into **5273 training images** and **637 testing images**. there are 5 classes of intrest including: *Covid(58 images),ARDS(2 images),Normal(1576),Other viral infections(1497 images) bacteria(2777 images)* The testing set has 165 batches while testing has 20 batches.","metadata":{}},{"cell_type":"code","source":"data_dir1= \"../input/coronahack/data/train\"  \ntrain_ds1= tf.keras.utils.image_dataset_from_directory(\n  data_dir1,\n  color_mode='grayscale',\n  labels='inferred',\n  label_mode='categorical',\n  image_size=(image_height,image_width),\n  validation_split=0,\n  seed=100)\ndata_dir2= \"../input/coronahack/data/test\"\n\ntest_ds1= tf.keras.utils.image_dataset_from_directory(\n  data_dir2,\n  color_mode='grayscale',\n  labels='inferred',\n  label_mode='categorical',\n  image_size=(299, 299),\n  validation_split=0,\n  seed=100)\n ","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:57:54.670603Z","iopub.execute_input":"2022-11-07T06:57:54.671004Z","iopub.status.idle":"2022-11-07T06:58:07.807108Z","shell.execute_reply.started":"2022-11-07T06:57:54.670970Z","shell.execute_reply":"2022-11-07T06:58:07.805883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The images belong to these 5 classes","metadata":{}},{"cell_type":"code","source":" # Class names\ntrain_ds1.class_names","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:58:07.809342Z","iopub.execute_input":"2022-11-07T06:58:07.809938Z","iopub.status.idle":"2022-11-07T06:58:07.819800Z","shell.execute_reply.started":"2022-11-07T06:58:07.809883Z","shell.execute_reply":"2022-11-07T06:58:07.818447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we collected details of the image, such as image counts in each group, training size and testing size etc. they are the details graphed below.","metadata":{}},{"cell_type":"code","source":"# Training labels\nlabs=list(map(fetch_labels,train_ds1.file_paths))\ntrainlabs=pd.DataFrame(labs,columns=[\"Labels\"])\n\n# testing labels\nlabs=list(map(fetch_labels,test_ds1.file_paths))\ntestlabs=pd.DataFrame(labs,columns=[\"Labels\"])\n\n# Overall image count\ndata_dirall=pathlib.Path(\"../input/coronahack/data\")\n\ndf=pd.DataFrame([[\"ARDS\",len(list(data_dirall.glob('*/ARDS/*.*')))],\n              [\"COVID-19\",len(list(data_dirall.glob('*/COVID-19/*.*')))],\n                [\"Normal\",len(list(data_dirall.glob('*/Normal/*.*')))],\n                [\"Other_viral_infections\",len(list(data_dirall.glob('*/Other_viral_infections/*.*')))],\n                [\"bacteria\",len(list(data_dirall.glob('*/bacteria/*.png')))]]\n             , columns=['Label','count'])\n\n# batch count\ndf1=pd.DataFrame([[\"Training\",len(train_ds1)],\n              [\"Testing\",len(test_ds1)]]\n             , columns=['Label','count'])","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:58:07.821291Z","iopub.execute_input":"2022-11-07T06:58:07.821624Z","iopub.status.idle":"2022-11-07T06:58:07.878252Z","shell.execute_reply.started":"2022-11-07T06:58:07.821595Z","shell.execute_reply":"2022-11-07T06:58:07.877085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-------------- Overall data ------------------------------------\\n\")\nprint(\"image count:\",len(list(data_dirall.glob('*/*/*.*'))))\nprint(\"ARDS image count:\",len(list(data_dirall.glob('*/ARDS/*.*'))))\nprint(\"Covid image count:\",len(list(data_dirall.glob('*/COVID-19/*.*'))))\nprint(\"Normal count:\",len(list(data_dirall.glob('*/Normal/*.*'))))\nprint(\"Other_viral_infections count:\",len(list(data_dirall.glob('*/Other_viral_infections/*.*'))))\nprint(\"bacteria image count:\",len(list(data_dirall.glob('*/bacteria/*.*'))))\n\nprint(\"\\n------------Train test split--------------------------------\\n\")\n\nprint(\"Training count:\",len(trainlabs))\nprint(\"Testing count:\",len(testlabs))\nprint(\"Number of batches training set:\",len(train_ds1))\nprint(\"Number of batches testing set:\",len(test_ds1)) ","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:58:07.879968Z","iopub.execute_input":"2022-11-07T06:58:07.880317Z","iopub.status.idle":"2022-11-07T06:58:07.961983Z","shell.execute_reply.started":"2022-11-07T06:58:07.880288Z","shell.execute_reply":"2022-11-07T06:58:07.960622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizing a the above summary statistics","metadata":{}},{"cell_type":"code","source":"figure,ax=plt.subplots(nrows=2,ncols=2,figsize=(12,12))\nsns.barplot(x = 'Label',y = 'count',data = df,ax=ax[0,0])\nsns.countplot(x='Labels',data=trainlabs,ax=ax[0,1])\nsns.countplot(x='Labels',data=testlabs,ax=ax[1,0])\nsns.barplot(x = 'Label',y = 'count',data = df1,ax=ax[1,1])\nax[0,0].set_title('Total Images')\nax[0,1].set_title('Training images')\nax[1,0].set_title('Testing images')\nax[1,1].set_title('Batch counts')\nfor p, label in zip(ax[0,1].patches, trainlabs['Labels'].value_counts().index):\n    ax[0,1].annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+20))\nfor p, label in zip(ax[1,0].patches, testlabs['Labels'].value_counts().index):\n    ax[1,0].annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+5))\nfor p, label in zip(ax[0,0].patches, df['count'].index):\n    ax[0,0].annotate(round(p.get_height()), (p.get_x()+0.25, p.get_height()+20))\nfor p, label in zip(ax[1,1].patches, df1['count'].index):\n    ax[1,1].annotate(round(p.get_height()), (p.get_x()+0.25, p.get_height()+5))\nax[0,0].set_xlabel(\"\")\nax[0,1].set_xlabel(\"\")\nax[1,0].set_xlabel(\"\")\nax[1,1].set_xlabel(\"\")\nax[0,0].tick_params(axis='x',rotation=90)\nax[0,1].tick_params(axis='x',rotation=90)\nax[1,0].tick_params(axis='x',rotation=90)\nplt.tight_layout()\nplt.savefig('coronahack data1.png')\n                    ","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:58:07.963648Z","iopub.execute_input":"2022-11-07T06:58:07.964014Z","iopub.status.idle":"2022-11-07T06:58:09.120719Z","shell.execute_reply.started":"2022-11-07T06:58:07.963981Z","shell.execute_reply":"2022-11-07T06:58:09.119546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A snapshot of the images","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfor images, labels in train_ds1.take(1):\n    for i in range(16):\n        ls=labels[i].numpy()\n        x=[j for j, y in enumerate(ls) if y == 1]\n        ax = plt.subplot(4, 4, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"),cmap='Greys_r')\n        plt.title(train_ds1.class_names[x[0]])\n        plt.axis(\"off\")\n        \nplt.savefig('coronahack data2.png')","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:58:09.122424Z","iopub.execute_input":"2022-11-07T06:58:09.122904Z","iopub.status.idle":"2022-11-07T06:58:12.322885Z","shell.execute_reply.started":"2022-11-07T06:58:09.122860Z","shell.execute_reply":"2022-11-07T06:58:12.321864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# performance configuration\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds1 = train_ds1.cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds1 = test_ds1.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:58:12.324444Z","iopub.execute_input":"2022-11-07T06:58:12.324853Z","iopub.status.idle":"2022-11-07T06:58:12.336905Z","shell.execute_reply.started":"2022-11-07T06:58:12.324806Z","shell.execute_reply":"2022-11-07T06:58:12.335532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"taking a sample of first 20 batches.","metadata":{}},{"cell_type":"code","source":"train_ds1=train_ds1.take(n)","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:58:12.338775Z","iopub.execute_input":"2022-11-07T06:58:12.339673Z","iopub.status.idle":"2022-11-07T06:58:12.403173Z","shell.execute_reply.started":"2022-11-07T06:58:12.339633Z","shell.execute_reply":"2022-11-07T06:58:12.401948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unlike the other data,the target class here is class 2. so we have to reconfigure the model.","metadata":{}},{"cell_type":"code","source":"modela=Sequential(name=\"Full_Model\")\n# Block 1\nmodela.add(Input(shape=(image_height,image_width,1),name=\"input\")) \nmodela.add(Conv2D(64, (3, 3), padding='same', activation='relu',name=\"block1_conv_1\"))\nmodela.add(BatchNormalization(name=\"block1_batch_normalization1\"))\nmodela.add(Conv2D(64, (3, 3), padding='same', activation='relu',name=\"block1_conv_2\"))\nmodela.add(BatchNormalization(name=\"block1_batch_normalization2\"))\nmodela.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block1_maxpool\"))\n\n# Block 2\n\nmodela.add(Conv2D(128, (3, 3), padding='same', activation='relu',name=\"block2_conv_1\"))\nmodela.add(BatchNormalization(name=\"block2_batch_normalization1\"))\nmodela.add(Conv2D(128, (3, 3), padding='same', activation='relu',name=\"block2_conv_2\"))\nmodela.add(BatchNormalization(name=\"block2_batch_normalization2\"))\nmodela.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block2_maxpool\"))\n          \n# Block 3\nmodela.add(Conv2D(256, (3, 3), padding='same', activation='relu',name=\"block3_conv_1\"))\nmodela.add(BatchNormalization(name=\"block3_batch_normalization1\"))\nmodela.add(Conv2D(256, (3, 3), padding='same', activation='relu',name=\"block3_conv_2\"))\nmodela.add(BatchNormalization(name=\"block3_batch_normalization2\"))\nmodela.add(Conv2D(256, (3, 3), padding='same', activation='relu',name=\"block3_conv_3\"))\nmodela.add(BatchNormalization(name=\"block3_batch_normalization3\"))\nmodela.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2),name=\"block3_maxpool\"))\n\n# Block 4\nmodela.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block4_conv_1\"))\nmodela.add(BatchNormalization(name=\"block4_batch_normalization1\"))\nmodela.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block4_conv_2\"))\nmodela.add(BatchNormalization(name=\"block4_batch_normalization2\"))\nmodela.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block4_conv_3\"))\nmodela.add(BatchNormalization(name=\"block4_batch_normalization3\"))\nmodela.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block4_maxpool\"))\n\n# fifth convolution layer\n\nmodela.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block5_conv_1\"))\nmodela.add(BatchNormalization(name=\"block5_batch_normalization1\"))\nmodela.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block5_conv_2\"))\nmodela.add(BatchNormalization(name=\"block5_batch_normalization2\"))\nmodela.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block5_conv_3\"))\nmodela.add(BatchNormalization(name=\"block5_batch_normalization3\"))\nmodela.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block5_maxpool\"))\n#  flatten\nmodela.add(Flatten(name=\"flatten_layer\"))\n\n# Dense connected layers\nmodela.add(Dense(units=64,activation=\"relu\"))\nmodela.add(Dense(units=64,activation=\"relu\"))\nmodela.add(Dense(units=5, activation=\"softmax\"))","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:58:12.406611Z","iopub.execute_input":"2022-11-07T06:58:12.407564Z","iopub.status.idle":"2022-11-07T06:58:12.999968Z","shell.execute_reply.started":"2022-11-07T06:58:12.407515Z","shell.execute_reply":"2022-11-07T06:58:12.998707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modela.compile(\n  optimizer='adam',\n  loss=\"categorical_crossentropy\",\n  metrics=[\"accuracy\",\n           tf.keras.metrics.Recall(name=\"Sensitivity\",class_id=1),\n           tf.keras.metrics.Recall(name=\"Specificity\",class_id=2),\n           tf.keras.metrics.Precision(name=\"Precision\",class_id=1),\n          tfa.metrics.F1Score(num_classes=5, average=\"micro\")])","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:58:13.001613Z","iopub.execute_input":"2022-11-07T06:58:13.002617Z","iopub.status.idle":"2022-11-07T06:58:13.033562Z","shell.execute_reply.started":"2022-11-07T06:58:13.002572Z","shell.execute_reply":"2022-11-07T06:58:13.032509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" fitting the model on this data.","metadata":{}},{"cell_type":"code","source":"hist=modela.fit(\n train_ds1,\n  validation_data=test_ds1,\n epochs=epochs)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:58:13.035526Z","iopub.execute_input":"2022-11-07T06:58:13.036704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Graphing accuracy","metadata":{}},{"cell_type":"code","source":"# Accuracy\nfigure,ax=plt.subplots(nrows=1,ncols=2,figsize=(15,6))\nax[0].plot(hist.history['accuracy'])\nax[0].plot(hist.history['val_accuracy'])\nax[0].set_title('model accuracy')\nax[0].set_ylabel('accuracy')\nax[0].set_xlabel('epoch')\nax[0].legend(['train', 'test'], loc='upper left')\n\n# loss\nax[1].plot(hist.history['loss'])\nax[1].plot(hist.history['val_loss'])\nax[1].set_title('model loss')\nax[1].set_ylabel('loss')\nax[1].set_xlabel('epoch')\nax[1].legend(['train', 'test'], loc='upper left')\nplt.savefig('coronahack data3.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nst = time.time()\nm2=modela.evaluate(test_ds1)[1:6]\net = time.time()\nelapsed_time=round((et - st)/len(testlabs),4)\nprint('Execution time:', elapsed_time, 'seconds')\nm2.append(elapsed_time)\nmod2=pd.DataFrame({\"Measure\":['Accuracy','Sensitivity','Specificty','Precision',\"F1-score\",\"Excecution time\"],\n    \"Corona hack data\":[np.round(float(i), 4) for i in m2]})\nmod2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"Coronahack_hist.pkl\",\"wb\") as file:\n    pickle.dump(hist,file)\n    \n#with open(\"original_hist.pkl\",\"rb\") as file:\n#    hist=pickle.load(file)\n\nmodela.save(\"Coronahack\")\n#train_ds.save(\"train_ds\")\n#test_ds.save(\"test_ds\")\n#model = keras.models.load_model('my_model')\n#train_ds=tf.data.Dataset.load(\"train_ds\")\n#test_ds=tf.data.Dataset.load(\"test_ds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset 2: Chest X-Ray Scans (Pneumonia)\n\nOver the past years, scores of research literature have shown respiratory infections to be common distress in both developing and developed countries (Keystone et al., 2013). The problem is more frequent in the upper respiratory tract. According to (Sails et al, 201). Respiratory infections contribute to nearly 20% of mortalities in infants (0-5years). COVID- 19 being a respiratory infection presents more problems, If not contained the disease has a high chance of inflating this percentage.  This dataset consists of Optical Coherence Tomography (OCT) and X-ray scans for normal and pneumonia cases in infants (0-5years). The motivation behind benchmarking the data here is to get a glimpse performance of the algorithms with infant data. \n\nData: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n","metadata":{}},{"cell_type":"markdown","source":"In total the data consists of **5856 images**, and which are split into **5216 training images** and **624 testing images** and **16 validation images**. there are 2 classes of intrest including:*Normal(1583) and pneumonia(4273 images)* The testing set has 163 batches while testing has 20 batches, the validation set has 1 batch. more breakdown can be found on the graphs","metadata":{}},{"cell_type":"code","source":"data_dir1= \"../input/chest-xray-pneumonia/chest_xray/train\" \ntrain_ds1= tf.keras.utils.image_dataset_from_directory(\n  data_dir1,\n  color_mode='grayscale',\n  labels='inferred',\n   label_mode='categorical',\n  image_size=(299,299),\n  validation_split=0,\n  seed=100)\ndata_dir2= \"../input/chest-xray-pneumonia/chest_xray/test\"\n\ntest_ds1= tf.keras.utils.image_dataset_from_directory(\n  data_dir2,\n  color_mode='grayscale',\n  labels='inferred',\n   label_mode='categorical',\n  image_size=(299, 299),\n  validation_split=0,\n  seed=100)\n\ndata_dir3= \"../input/chest-xray-pneumonia/chest_xray/val\"\nvalid_ds1= tf.keras.utils.image_dataset_from_directory(\n  data_dir3,\n  color_mode='grayscale',\n  labels='inferred',\n   label_mode='categorical',\n  image_size=(299, 299),\n  validation_split=0,\n  seed=100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training labels\nlabs=list(map(fetch_labels,train_ds1.file_paths))\ntrainlabs=pd.DataFrame(labs,columns=[\"Labels\"])\n\n# testing labels\nlabs=list(map(fetch_labels,test_ds1.file_paths))\ntestlabs=pd.DataFrame(labs,columns=[\"Labels\"])\n\nlabs=list(map(fetch_labels,valid_ds1.file_paths))\nvalidlabs=pd.DataFrame(labs,columns=[\"Labels\"])\n\n# Overall image count\ndata_dirall=pathlib.Path(\"../input/chest-xray-pneumonia/chest_xray/\")\ndf=pd.DataFrame([[\"Normal\",len(list(data_dirall.glob('*/NORMAL/*.*')))],\n              [\"Pneumonia\",len(list(data_dirall.glob('*/PNEUMONIA/*.*')))]]\n             , columns=['Label','count'])\n# batch count\ndf1=pd.DataFrame([[\"Training\",len(train_ds1)],\n              [\"Testing\",len(test_ds1)],\n                 [\"Validation\",len(valid_ds1)]],\n                 columns=['Label','count'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-------------- Overall data ------------------------------------\\n\")\nprint(\"image count:\",len(list(data_dirall.glob('*/NORMAL/*.*')))+len(list(data_dirall.glob('*/PNEUMONIA/*.*'))))\nprint(\"Normal image count:\",len(list(data_dirall.glob('*/NORMAL/*.*'))))\nprint(\"Pneumonia image count:\",len(list(data_dirall.glob('*/PNEUMONIA/*.*'))))\n\nprint(\"\\n------------Train test split--------------------------------\\n\")\nprint(\"Training count:\",len(trainlabs))\nprint(\"Testing count:\",len(testlabs))\nprint(\"Validation count:\",len(validlabs))\nprint(\"Number of batches training set:\",len(train_ds1))\nprint(\"Number of batches testing set:\",len(test_ds1)) \nprint(\"Number of batches Validation set:\",len(valid_ds1)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure,ax=plt.subplots(nrows=2,ncols=2,figsize=(12,10))\nsns.barplot(x = 'Label',y = 'count',data = df,ax=ax[0,0])\nsns.countplot(x='Labels',data=trainlabs,ax=ax[0,1])\nsns.countplot(x='Labels',data=testlabs,ax=ax[1,0])\nsns.barplot(x = 'Label',y = 'count',data = df1,ax=ax[1,1])\nax[0,0].set_title('Total Images')\nax[0,1].set_title('Training images')\nax[1,0].set_title('Testing images')\nax[1,1].set_title('Batch counts')\nfor p, label in zip(ax[0,1].patches, trainlabs['Labels'].value_counts().index):\n    ax[0,1].annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+20))\nfor p, label in zip(ax[1,0].patches, testlabs['Labels'].value_counts().index):\n    ax[1,0].annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+20))\nfor p, label in zip(ax[0,0].patches, df['count'].index):\n    ax[0,0].annotate(round(p.get_height()), (p.get_x()+0.25, p.get_height()+20))\nfor p, label in zip(ax[1,1].patches, df1['count'].index):\n    ax[1,1].annotate(round(p.get_height()), (p.get_x()+0.25, p.get_height()+5))\nax[0,0].set_xlabel(\"\")\nax[0,1].set_xlabel(\"\")\nax[1,0].set_xlabel(\"\")\nax[1,1].set_xlabel(\"\")\nplt.savefig('xray images data1.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A snapshot of images","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfor images, labels in train_ds1.take(1):\n    for i in range(16):\n        ls=labels[i].numpy()\n        x=[j for j, y in enumerate(ls) if y == 1]\n        ax = plt.subplot(4, 4, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"),cmap='Greys_r')\n        plt.title(train_ds1.class_names[x[0]])\n        plt.axis(\"off\")\nplt.savefig('xray images data2.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds1.class_names ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# performance configuration\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds1 = train_ds1.cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds1 = test_ds1.cache().prefetch(buffer_size=AUTOTUNE)\nvalid_ds1 = valid_ds1.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a sample of training batches","metadata":{}},{"cell_type":"code","source":"train_ds1=train_ds1.take(n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compile model with new label class arragement i.e 1= target category,0 is the other category.","metadata":{}},{"cell_type":"code","source":"model.compile(\n  optimizer='adam',\n  loss=\"categorical_crossentropy\",\n  metrics=[\"accuracy\",\n           tf.keras.metrics.Recall(name=\"Sensitivity\",class_id=1),\n           tf.keras.metrics.Recall(name=\"Specificity\",class_id=0),\n           tf.keras.metrics.Precision(name=\"Precision\",class_id=1),\n          tfa.metrics.F1Score(num_classes=2, average=\"micro\")])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist=model.fit(\n train_ds1,\n  validation_data=test_ds1,\n epochs=epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy\nfigure,ax=plt.subplots(nrows=1,ncols=2,figsize=(15,6))\nax[0].plot(hist.history['accuracy'])\nax[0].plot(hist.history['val_accuracy'])\nax[0].set_title('model accuracy')\nax[0].set_ylabel('accuracy')\nax[0].set_xlabel('epoch')\nax[0].legend(['train', 'test'], loc='upper left')\n\n# loss\nax[1].plot(hist.history['loss'])\nax[1].plot(hist.history['val_loss'])\nax[1].set_title('model loss')\nax[1].set_ylabel('loss')\nax[1].set_xlabel('epoch')\nax[1].legend(['train', 'test'], loc='upper left')\nplt.savefig('xray images data3.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time.time()\nm3=model.evaluate(test_ds1)[1:6]\net = time.time()\nelapsed_time=round((et - st)/len(testlabs),4)\nprint('Execution time:', elapsed_time, 'seconds')\nm3.append(elapsed_time)\nmod3=pd.DataFrame({\"Measure\":['Accuracy','Sensitivity','Specificty','Precision',\"F1-score\",\"Excecution time\"],\n    \"Chest X-Ray Scans\":[np.round(float(i), 4) for i in m3]})\nmod3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"Chestxray_hist.pkl\",\"wb\") as file:\n    pickle.dump(hist,file)\n    \n#with open(\"original_hist.pkl\",\"rb\") as file:\n#    hist=pickle.load(file)\n\nmodel.save(\"Chestxray\")\n#train_ds.save(\"train_ds\")\n#test_ds.save(\"test_ds\")\n#model = keras.models.load_model('my_model')\n#train_ds=tf.data.Dataset.load(\"train_ds\")\n#test_ds=tf.data.Dataset.load(\"test_ds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset 4: COVID19+PNEUMONIA+NORMAL Chest X-Ray Images\nAccording to (Gavrilov et al, 2018), the training set utilized to develop ML models greatly affects the model accuracy. In environments where there are no sufficient outcome cases (where only a few of the sampled patients are infected), the algorithms learn more from normal people and have little information about outcome cases. The consequence of this is having a model which diagnoses the absence of COVID-19 more accurately than it does with presence (higher specificity but low sensitivity). This benchmark data consists of **5228 images**  from 3 study group, the three includes; a control group with **1802 X-ray images from normal people**, an experimental group with **1626 COVID-19 X-ray images**, and **1800 images from people with other Pneumonia infections (Shastri et al., 2022)**.Here we aim at running the model with such balanced groups to make to see the difference.\n\nData link: https://www.kaggle.com/datasets/sachinkumar413/covid-pneumonia-normal-chest-xray-images\n\nThe training set has 131 batches while testing has 33 batches more breakdown can be found on the graphs","metadata":{}},{"cell_type":"code","source":"data_dir1= \"../input/covid-pneumonia-normal-chest-xray-images\"\ntrain_ds1= tf.keras.utils.image_dataset_from_directory(\n  data_dir1,\n  color_mode='grayscale',\n  labels='inferred',\n  label_mode='categorical',\n  subset=\"training\",\n  image_size=(image_height,image_width),\n  validation_split=0.20,\n  seed=100)\n\ntest_ds1= tf.keras.utils.image_dataset_from_directory(\n  data_dir1,\n  color_mode='grayscale',\n  labels='inferred',\n  label_mode='categorical',\n  subset=\"validation\",\n  image_size=(299, 299),\n  validation_split=0.20,\n  seed=100)\n\n\n\n#Loading without the third group\n\ndata_dir2= \"../input/covidpneumonianormal\"\ntrain_ds2= tf.keras.utils.image_dataset_from_directory(\n  data_dir2,\n  color_mode='grayscale',\n  labels='inferred',\n  label_mode='categorical',\n  subset=\"training\",\n  image_size=(image_height,image_width),\n  validation_split=0.20,\n  seed=100)\n\ntest_ds2= tf.keras.utils.image_dataset_from_directory(\n  data_dir2,\n  color_mode='grayscale',\n  labels='inferred',\n  label_mode='categorical',\n  subset=\"validation\",\n  image_size=(299, 299),\n  validation_split=0.20,\n  seed=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training labels\nlabs=list(map(fetch_labels,train_ds1.file_paths))\ntrainlabs=pd.DataFrame(labs,columns=[\"Labels\"])\n\n# testing labels\nlabs=list(map(fetch_labels,test_ds1.file_paths))\ntestlabs=pd.DataFrame(labs,columns=[\"Labels\"])\n\n# Overall image count\ndata_dirall=pathlib.Path(\"../input/covid-pneumonia-normal-chest-xray-images\")\ndf=pd.DataFrame([[\"Normal\",len(list(data_dirall.glob('NORMAL/*.*')))],\n              [\"Pneumonia\",len(list(data_dirall.glob('PNEUMONIA/*.*')))],\n                [\"Covid\",len(list(data_dirall.glob('COVID/*.*')))]]\n             , columns=['Label','count'])\n# batch count\ndf1=pd.DataFrame([[\"Training\",len(train_ds1)],\n              [\"Testing\",len(test_ds1)]]\n             , columns=['Label','count'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-------------- Overall data ------------------------------------\\n\")\nprint(\"image count:\",len(list(data_dirall.glob('*/*.*'))))\nprint(\"Normal image count:\",len(list(data_dirall.glob('NORMAL/*.*'))))\nprint(\"Pneumonia image count:\",len(list(data_dirall.glob('PNEUMONIA/*.*'))))\nprint(\"Covid image count:\",len(list(data_dirall.glob('COVID/*.*'))))\nprint(\"\\n------------Train test split--------------------------------\\n\")\nprint(\"Training count:\",len(trainlabs))\nprint(\"Testing count:\",len(testlabs))\nprint(\"Number of batches training set:\",len(train_ds1))\nprint(\"Number of batches testing set:\",len(test_ds1)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure,ax=plt.subplots(nrows=2,ncols=2,figsize=(12,10))\nsns.barplot(x = 'Label',y = 'count',data = df,ax=ax[0,0])\nsns.countplot(x='Labels',data=trainlabs,ax=ax[0,1])\nsns.countplot(x='Labels',data=testlabs,ax=ax[1,0])\nsns.barplot(x = 'Label',y = 'count',data = df1,ax=ax[1,1])\nax[0,0].set_title('Total Images')\nax[0,1].set_title('Training images')\nax[1,0].set_title('Testing images')\nax[1,1].set_title('Batch counts')\nfor p, label in zip(ax[0,1].patches, trainlabs['Labels'].value_counts().index):\n    ax[0,1].annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+20))\nfor p, label in zip(ax[1,0].patches, testlabs['Labels'].value_counts().index):\n    ax[1,0].annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+13))\nfor p, label in zip(ax[0,0].patches, df['count'].index):\n    ax[0,0].annotate(round(p.get_height()), (p.get_x()+0.25, p.get_height()+20))\nfor p, label in zip(ax[1,1].patches, df1['count'].index):\n    ax[1,1].annotate(round(p.get_height()), (p.get_x()+0.25, p.get_height()+5))\nax[0,0].set_xlabel(\"\")\nax[0,1].set_xlabel(\"\")\nax[1,0].set_xlabel(\"\")\nax[1,1].set_xlabel(\"\")\n    \nplt.savefig('covid pneumonia Normal data1.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"a snapshot of the data","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfor images, labels in train_ds1.take(1):\n    for i in range(16):\n        ls=labels[i].numpy()\n        x=[j for j, y in enumerate(ls) if y == 1]\n        ax = plt.subplot(4, 4, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"),cmap='Greys_r')\n        plt.title(train_ds1.class_names[x[0]])\n        plt.axis(\"off\")\nplt.savefig('covid pneumonia Normal data2.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds2.class_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# performance configuration\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds1 = train_ds2.cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds1 = test_ds2.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds1=train_ds1.take(n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n  optimizer='adam',\n  loss=\"categorical_crossentropy\",\n  metrics=[\"accuracy\",\n           tf.keras.metrics.Recall(name=\"Sensitivity\",class_id=0),\n           tf.keras.metrics.Recall(name=\"Specificity\",class_id=1),\n           tf.keras.metrics.Precision(name=\"Precision\",class_id=0),\n          tfa.metrics.F1Score(num_classes=3, average=\"micro\")])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist=model.fit(\n train_ds1,\n  validation_data=test_ds1,\n epochs=epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"graphing accuracies","metadata":{}},{"cell_type":"code","source":"# Accuracy\nfigure,ax=plt.subplots(nrows=1,ncols=2,figsize=(15,6))\nax[0].plot(hist.history['accuracy'])\nax[0].plot(hist.history['val_accuracy'])\nax[0].set_title('model accuracy')\nax[0].set_ylabel('accuracy')\nax[0].set_xlabel('epoch')\nax[0].legend(['train', 'test'], loc='upper left')\n\n# loss\nax[1].plot(hist.history['loss'])\nax[1].plot(hist.history['val_loss'])\nax[1].set_title('model loss')\nax[1].set_ylabel('loss')\nax[1].set_xlabel('epoch')\nax[1].legend(['train', 'test'], loc='upper left')\nplt.savefig('covid pneumonia Normal data3.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time.time()\nm5=model.evaluate(test_ds1)[1:6]\net = time.time()\nelapsed_time=round((et - st)/len(testlabs),4)\nprint('Execution time:', elapsed_time, 'seconds')\nm5.append(elapsed_time)\nmod5=pd.DataFrame({\"Measure\":['Accuracy','Sensitivity','Specificty','Precision',\"F1-score\",\"Excecution time\"],\n    \"COvid+Pneumonia+Normal data\":[np.round(float(i), 4) for i in m5]})\nmod5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"covidpneumoniaNormal_hist.pkl\",\"wb\") as file:\n    pickle.dump(hist,file)\n    \n#with open(\"original_hist.pkl\",\"rb\") as file:\n#    hist=pickle.load(file)\n\nmodel.save(\"covidpneumoniaNormal\")\n#train_ds.save(\"train_ds\")\n#test_ds.save(\"test_ds\")\n#model = keras.models.load_model('my_model')\n#train_ds=tf.data.Dataset.load(\"train_ds\")\n#test_ds=tf.data.Dataset.load(\"test_ds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# comparing perfomance of our approach on varied datasets","metadata":{}},{"cell_type":"code","source":"alldatasets=[mod2,mod3,mod5]\nreduce(lambda  left,right: pd.merge(left,right,on=['Measure'],\n                                            how='left'),alldatasets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}