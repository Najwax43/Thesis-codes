{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Change tensorflow version from 2.6.4 to 2.9.1\n\nI noticed that the default version of tensorflow (2.6.4) has a problem of printing unecessary clean up messages. for this reason I upgraded to tensorflow 2.9.1","metadata":{}},{"cell_type":"code","source":"!pip uninstall tensorflow --yes\n!pip install tensorflow_decision_forests\n!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 --yes\n!pip install kerassurgeon ","metadata":{"execution":{"iopub.status.busy":"2022-11-07T14:34:12.544121Z","iopub.execute_input":"2022-11-07T14:34:12.545015Z","iopub.status.idle":"2022-11-07T14:36:30.776248Z","shell.execute_reply.started":"2022-11-07T14:34:12.544921Z","shell.execute_reply":"2022-11-07T14:36:30.775041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Library\nhere are the required packages/modules we will use to perform the tasks","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport gc\nimport numpy as np\nimport pandas as pd\nimport glob\nimport pathlib\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\n#from keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow.keras import datasets, layers, models,Input,Model \nimport tensorflow_datasets as tfds\nfrom keras.models import Sequential\nfrom keras.layers import  Bidirectional, Conv2D, BatchNormalization, MaxPooling2D, Flatten, LSTM, Dense, Lambda, Dropout,Reshape\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom tensorflow.keras.metrics import Accuracy, Recall,Precision\nfrom sklearn.tree import DecisionTreeClassifier as Decisiontree\nfrom sklearn.svm import SVC as Supportvectorclassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport time\nfrom functools import reduce\nfrom kerassurgeon.operations import delete_layer, insert_layer\nfrom keras.utils import to_categorical\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2022-11-07T14:36:30.779463Z","iopub.execute_input":"2022-11-07T14:36:30.779897Z","iopub.status.idle":"2022-11-07T14:36:36.520043Z","shell.execute_reply.started":"2022-11-07T14:36:30.779852Z","shell.execute_reply":"2022-11-07T14:36:36.519101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just checking if tensorflow was sucessfully upgraded.","metadata":{}},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-11-07T14:36:36.521288Z","iopub.execute_input":"2022-11-07T14:36:36.522270Z","iopub.status.idle":"2022-11-07T14:36:36.529961Z","shell.execute_reply.started":"2022-11-07T14:36:36.522231Z","shell.execute_reply":"2022-11-07T14:36:36.527379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some parameter settings\n\nbatch_size=32\nimage_height=299\nimage_width=299\nn=1000\nepochs=100","metadata":{"execution":{"iopub.status.busy":"2022-11-07T14:36:36.532313Z","iopub.execute_input":"2022-11-07T14:36:36.533164Z","iopub.status.idle":"2022-11-07T14:36:36.544350Z","shell.execute_reply.started":"2022-11-07T14:36:36.533128Z","shell.execute_reply":"2022-11-07T14:36:36.543391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to get labs \ndef fetch_labels(filepath):\n    return str(filepath).split('/')[-2]\n","metadata":{"execution":{"iopub.status.busy":"2022-11-07T06:57:53.856505Z","iopub.execute_input":"2022-11-07T06:57:53.857150Z","iopub.status.idle":"2022-11-07T06:57:53.869427Z","shell.execute_reply.started":"2022-11-07T06:57:53.857103Z","shell.execute_reply":"2022-11-07T06:57:53.868501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Dataset 3: Patch Camelyon","metadata":{}},{"cell_type":"code","source":"ds,info=tfds.load(\"PatchCamelyon\", with_info=True,as_supervised = True)  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(info)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = ds['train'].batch(batch_size=32)\nvalid_data = ds['validation'].batch(batch_size=32)\ntest_data = ds['test'].batch(batch_size=32)\ntrain_ds1=train_data.take(n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lebb=['No metastatic tissue','metastatic tissue']\n\ndef to_lab(x):\n    return lebb[round(x)]\n\ndef to_lab1(batched):\n    labs=np.array([])\n\n    for batch in batched:\n        images,labels=batch\n        lb=np.array(list(map(to_lab,labels.numpy())))\n        labs= np.append(labs,lb)\n    return labs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainlabs=to_lab1(train_data)\ntestlabs=to_lab1(test_data)\nvalidlabs=to_lab1(valid_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fulld=pd.concat([pd.DataFrame({'Label': trainlabs}),\n           pd.DataFrame({'Label': testlabs}),\n           pd.DataFrame({'Label': validlabs})])\n\nfigure,ax=plt.subplots(nrows=2,ncols=3,figsize=(12,12))\n\nsns.countplot(x = 'Label',data = pd.concat([pd.DataFrame({'Label': trainlabs}),\n           pd.DataFrame({'Label': testlabs}),\n           pd.DataFrame({'Label': validlabs})]),ax=ax[0,0])\n\nsns.countplot(x = 'Label',data = pd.DataFrame({'Label': trainlabs}),ax=ax[0,1])\nsns.countplot(x='Label',data=pd.DataFrame({'Label': testlabs}),ax=ax[0,2])\nsns.countplot(x='Label',data=pd.DataFrame({'Label': validlabs}),ax=ax[1,0])\nsns.barplot(x = 'Label',y = 'count',\n            data = pd.DataFrame({'Label':['Training',\n                                          'Testing',\n                                          'Validation'],\n                                 'count': [len(train_data),\n                                           len(test_data),\n                                           len(valid_data)]}),ax=ax[1,1])\nax[0,0].set_title('Total Images')\nax[0,1].set_title('Training images')\nax[0,2].set_title('Testing images')\nax[1,0].set_title('validation images')\nax[1,1].set_title('Batch counts')\n\nfor p, label in zip(ax[0,0].patches, fulld['Label'].value_counts().index):\n    ax[0,0].annotate(round(p.get_height()), (p.get_x()+0.25, p.get_height()+5))\n                    \nfor p, label in zip(ax[0,1].patches, pd.value_counts(trainlabs).index):\n    ax[0,1].annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+20))\n    \nfor p, label in zip(ax[0,2].patches, pd.value_counts(trainlabs).index):\n    ax[0,2].annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+20))\n    \nfor p, label in zip(ax[1,0].patches, pd.value_counts(testlabs).index):\n    ax[1,0].annotate(round(p.get_height()), (p.get_x()+0.25, p.get_height()+20))\n    \nfor p, label in zip(ax[1,1].patches, pd.DataFrame({'Label':['Training',\n                                          'Testing',\n                                          'Validation'],\n                                 'count': [len(train_data),\n                                           len(test_data),\n                                           len(valid_data)]})['Label'].value_counts().index):\n    ax[1,1].annotate(round(p.get_height()), (p.get_x()+0.25, p.get_height()+20))\n    \n                    \nax[0,0].set_xlabel(\"\")\nax[0,1].set_xlabel(\"\")\nax[0,2].set_xlabel(\"\")\nax[1,0].set_xlabel(\"\")\nax[1,1].set_xlabel(\"\")\nplt.savefig('Patch camelyon data1.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(13, 13))\nfor data in train_ds1.take(1):\n    images, labels = data\n    for i in range(16):\n        ls=labels[i].numpy()\n        ax = plt.subplot(4, 4, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"),cmap='Greys_r')\n        plt.title(lebb[ls])\n        plt.axis(\"off\")\nplt.savefig('Patch camelyon data2.png')\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_categorical_crossentr(image, label):\n    return tf.image.rgb_to_grayscale(image),tf.one_hot(label, 2,dtype=tf.int64)\n\ntrain_data = ds['train'].map(to_categorical_crossentr).batch(batch_size=32)\nvalid_data = ds['validation'].map(to_categorical_crossentr).batch(batch_size=32)\ntest_data = ds['test'].map(to_categorical_crossentr).batch(batch_size=32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# performance configuration\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds1 = train_data.cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds1 = test_data.cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds1 = test_data.cache().prefetch(buffer_size=AUTOTUNE) \ntrain_ds1=train_ds1.take(n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model10=Sequential(name=\"Full_Model10\")\n# Block 1\nmodel10.add(Input(shape=(96,96,1),name=\"input\"))\nmodel10.add(Conv2D(64, (3, 3), padding='same', activation='relu',name=\"block1_conv_1\"))\nmodel10.add(BatchNormalization(name=\"block1_batch_normalization1\"))\nmodel10.add(Conv2D(64, (3, 3), padding='same', activation='relu',name=\"block1_conv_2\"))\nmodel10.add(BatchNormalization(name=\"block1_batch_normalization2\"))\nmodel10.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block1_maxpool\"))\n\n# Block 2\n\nmodel10.add(Conv2D(128, (3, 3), padding='same', activation='relu',name=\"block2_conv_1\"))\nmodel10.add(BatchNormalization(name=\"block2_batch_normalization1\"))\nmodel10.add(Conv2D(128, (3, 3), padding='same', activation='relu',name=\"block2_conv_2\"))\nmodel10.add(BatchNormalization(name=\"block2_batch_normalization2\"))\nmodel10.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block2_maxpool\"))\n          \n# Block 3\nmodel10.add(Conv2D(256, (3, 3), padding='same', activation='relu',name=\"block3_conv_1\"))\nmodel10.add(BatchNormalization(name=\"block3_batch_normalization1\"))\nmodel10.add(Conv2D(256, (3, 3), padding='same', activation='relu',name=\"block3_conv_2\"))\nmodel10.add(BatchNormalization(name=\"block3_batch_normalization2\"))\nmodel10.add(Conv2D(256, (3, 3), padding='same', activation='relu',name=\"block3_conv_3\"))\nmodel10.add(BatchNormalization(name=\"block3_batch_normalization3\"))\nmodel10.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2),name=\"block3_maxpool\"))\n\n\n# Block 4\nmodel10.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block4_conv_1\"))\nmodel10.add(BatchNormalization(name=\"block4_batch_normalization1\"))\nmodel10.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block4_conv_2\"))\nmodel10.add(BatchNormalization(name=\"block4_batch_normalization2\"))\nmodel10.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block4_conv_3\"))\nmodel10.add(BatchNormalization(name=\"block4_batch_normalization3\"))\nmodel10.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block4_maxpool\"))\n\n# fifth convolution layer\n\nmodel10.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block5_conv_1\"))\nmodel10.add(BatchNormalization(name=\"block5_batch_normalization1\"))\nmodel10.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block5_conv_2\"))\nmodel10.add(BatchNormalization(name=\"block5_batch_normalization2\"))\nmodel10.add(Conv2D(512, (3, 3), padding='same', activation='relu',name=\"block5_conv_3\"))\nmodel10.add(BatchNormalization(name=\"block5_batch_normalization3\"))\nmodel10.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2),name=\"block5_maxpool\"))\n#  flatten\nmodel10.add(Flatten(name=\"flatten_layer\"))\n\n# Dense connected layers\nmodel10.add(Dense(units=64,activation=\"relu\"))\nmodel10.add(Dense(units=64,activation=\"relu\"))\nmodel10.add(Dense(units=2, activation=\"softmax\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model10.compile(\n  optimizer='adam',\n  loss=\"categorical_crossentropy\",\n  metrics=[\"accuracy\",\n           tf.keras.metrics.Recall(name=\"Sensitivity\",class_id=0),\n           tf.keras.metrics.Recall(name=\"Specificity\",class_id=1),\n           tf.keras.metrics.Precision(name=\"Precision\",class_id=0),\n          tfa.metrics.F1Score(num_classes=2, average=\"micro\")])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist=model10.fit(\n train_ds1,\n  validation_data=test_ds1,\n epochs=epochs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accuracy\nfigure,ax=plt.subplots(nrows=1,ncols=2,figsize=(15,6))\nax[0].plot(hist.history['accuracy'])\nax[0].plot(hist.history['val_accuracy'])\nax[0].set_title('model accuracy')\nax[0].set_ylabel('accuracy')\nax[0].set_xlabel('epoch')\nax[0].legend(['train', 'test'], loc='upper left')\n\n# loss\nax[1].plot(hist.history['loss'])\nax[1].plot(hist.history['val_loss'])\nax[1].set_title('model loss')\nax[1].set_ylabel('loss')\nax[1].set_xlabel('epoch')\nax[1].legend(['train', 'test'], loc='upper left')\nplt.savefig('Patch camelyon data3.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#m1=model.evaluate(test_ds)\nst = time.time()\nm1=model10.evaluate(test_ds1)[1:6]\net = time.time()\nelapsed_time=round((et - st)/len(testlabs),4)\nprint('Execution time:', elapsed_time, 'seconds')\nm1.append(elapsed_time)\nmod4=pd.DataFrame({\"Measure\":['Accuracy','Sensitivity','Specificty','Precision',\"F1-score\",\"Excecution time\"],\n    \"Patch camelyon data\":[np.round(float(i), 4) for i in m1]})\nmod4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model10.save(\"patchcamelyon\")\n\nwith open(\"patchcamelyon.pkl\",\"wb\") as file:\n    pickle.dump(hist,file) ","metadata":{},"execution_count":null,"outputs":[]}]}